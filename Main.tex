%% LyX 2.1.4 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[11pt,american,czech]{book}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[a4paper]{geometry}
\geometry{verbose,tmargin=4cm,bmargin=3cm,lmargin=3cm,rmargin=2cm,headheight=0.8cm,headsep=1cm,footskip=0.5cm}
\pagestyle{headings}
\setcounter{secnumdepth}{3}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{comment}
\usepackage{float}
\restylefloat{table}
\usepackage{amssymb}
\usepackage{booktabs}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\newenvironment{lyxlist}[1]
{\begin{list}{}
{\settowidth{\labelwidth}{#1}
 \setlength{\leftmargin}{\labelwidth}
 \addtolength{\leftmargin}{\labelsep}
 \renewcommand{\makelabel}[1]{##1\hfil}}}
{\end{list}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
%% Font setup: please leave the LyX font settings all set to 'default'
%% if you want to use any of these packages:

%% Use Times New Roman font for text and Belleek font for math
%% Please make sure that the 'esint' package is turned off in the
%% 'Math options' page.
\usepackage[varg]{txfonts}

%% Use Utopia text with Fourier-GUTenberg math
%\usepackage{fourier}

%% Bitstream Charter text with Math Design math
%\usepackage[charter]{mathdesign}

%%---------------------------------------------------------------------

%% Make the multiline figure/table captions indent so that the second
%% line "hangs" right below the first one.
%\usepackage[format=hang]{caption}

%% Indent even the first paragraph in each section
\usepackage{indentfirst}

%%---------------------------------------------------------------------

%% Disable page numbers in the TOC. LOF, LOT (TOC automatically
%% adds \thispagestyle{chapter} if not overriden
%\addtocontents{toc}{\protect\thispagestyle{empty}}
%\addtocontents{lof}{\protect\thispagestyle{empty}}
%\addtocontents{lot}{\protect\thispagestyle{empty}}

%% Shifts the top line of the TOC (not the title) 1cm upwards 
%% so that the whole TOC fits on 1 page. Additional page size
%% adjustment is performed at the point where the TOC
%% is inserted.
%\addtocontents{toc}{\protect\vspace{-1cm}}

%%---------------------------------------------------------------------

% completely avoid orphans (first lines of a new paragraph on the bottom of a page)
\clubpenalty=9500

% completely avoid widows (last lines of paragraph on a new page)
\widowpenalty=9500

% disable hyphenation of acronyms
\hyphenation{CDFA HARDI HiPPIES IKEM InterTrack MEGIDDO MIMD MPFA DICOM ASCLEPIOS MedInria}

%%---------------------------------------------------------------------

%% Print out all vectors in bold type instead of printing an arrow above them
\renewcommand{\vec}[1]{\boldsymbol{#1}}

% Replace standard \cite by the parenthetical variant \citep
%\renewcommand{\cite}{\citep}

\makeatother

\usepackage{babel}
\begin{document}
\def\documentdate{2. \v{c}ervence 2018}

%%\def\documentdate{\today}

\pagestyle{empty}
{\centering

\noindent %
\begin{minipage}[c]{3cm}%
\noindent \begin{center}
\includegraphics[width=3cm,height=3cm,keepaspectratio]{Images/TITLE/cvut}
\par\end{center}%
\end{minipage}%
\begin{minipage}[c]{0.6\linewidth}%
\begin{center}
\textsc{\large{}České vysoké učení technické v Praze}{\large{}}\\
{\large{}Fakulta jaderná a fyzikálně inženýrská}
\par\end{center}%
\end{minipage}%
\begin{minipage}[c]{3cm}%
\noindent \begin{center}
\includegraphics[width=3cm,height=3cm,keepaspectratio]{Images/TITLE/fjfi}
\par\end{center}%
\end{minipage}

\vspace{3cm}


\textbf{\huge{}Těžba dat z experimentů na tokamaku COMPASS}{\huge \par}

\vspace{1cm}


\selectlanguage{american}%
\textbf{\huge{}Data mining on the COMPASS tokamak experiments}{\huge \par}

\selectlanguage{czech}%
\vspace{2cm}


{\large{}Bakalářská práce}{\large \par}

}

\vfill{}

\begin{lyxlist}{MMMMMMMMM}
\begin{singlespace}
\item [{Autor:}] \textbf{Matěj Zorek}
\item [{Vedoucí~práce:}] \textbf{Ing. Vít Škvára}
\item [{Konzultant:}] \textbf{Ing. Jakub Urban, PhD}
\item [{Akademický~rok:}] 2017/2018\end{singlespace}

\end{lyxlist}
\newpage{}

~\newpage{}

~

\vfill{}


\begin{center}
- Zadání práce -
\par\end{center}

\vfill{}


~\newpage{}

~

\vfill{}


\begin{center}
- Zadání práce (zadní strana) -
\par\end{center}

\vfill{}


~\newpage{}

\noindent \emph{\Large{}Poděkování:}{\Large \par}

\noindent Chtěl bych zde poděkovat především svému školiteli Ing. Vítovi Škvárovi za pečlivost, ochotu, vstřícnost a odborné i lidské zázemí při vedení mé bakalářské práce. Dále děkuji svému konzultantovi Ing. Jakubu Urbanovi, PhD., za pomoc nejden v začátcích řešení problému, jímž se tato práce zabívá. V neposlední řadě bych chtěl poděkovat Ing. Ondřeji Groverovi za pomoc s fyzikální složkou věci.

\vfill

\noindent \emph{\Large{}Čestné prohlášení:}{\Large \par}

\noindent Prohlašuji, že jsem tuto práci vypracoval samostatně a uvedl
jsem všechnu použitou literaturu.

\bigskip{}


\noindent V Praze dne \documentdate\hfill{}Matěj Zorek

\vspace{2cm}


\newpage{}

~\newpage{}

\begin{onehalfspace}
\noindent \emph{Název práce:}

\noindent \textbf{Těžba dat z experimentů na tokamaku COMPASS}
\end{onehalfspace}

\bigskip{}


\noindent \emph{Autor:} Matěj Zorek

\bigskip{}


\noindent \emph{Obor:} Matematické inženýrstvý\bigskip{}


\noindent \emph{Zaměření:} Aplikované matematicko-stochastické metody

\bigskip{}


\noindent \emph{Druh práce:} Bakalářská práce

\bigskip{}


\noindent \emph{Vedoucí práce:} Ing. Vít Škvára, Ústav fyziky plazmatu, AV ČR
Za Slovankou 1782/3
182 00 Praha 8

\bigskip{}


\noindent \emph{Konzultant:} Ing. Jakub Urban, PhD., Ústav fyziky plazmatu, AV ČR, Za Slovankou 1782/3, 182 00 Praha 8

\bigskip{}


\noindent \emph{Abstrakt:} Abstrakt max. na 10 řádků. Abstrakt max.
na 10 řádků. Abstrakt max. na 10 řádků. Abstrakt max. na 10 řádků.
Abstrakt max. na 10 řádků. Abstrakt max. na 10 řádků. Abstrakt max.
na 10 řádků. Abstrakt max. na 10 řádků. Abstrakt max. na 10 řádků.
Abstrakt max. na 10 řádků. Abstrakt max. na 10 řádků. Abstrakt max.
na 10 řádků. Abstrakt max. na 10 řádků. Abstrakt max. na 10 řádků.
Abstrakt max. na 10 řádků. Abstrakt max. na 10 řádků. Abstrakt max.
na 10 řádků. Abstrakt max. na 10 řádků. Abstrakt max. na 10 řádků.
Abstrakt max. na 10 řádků. Abstrakt max. na 10 řádků. Abstrakt max.
na 10 řádků. Abstrakt max. na 10 řádků. Abstrakt max. na 10 řádků.
Abstrakt max. na 10 řádků. Abstrakt max. na 10 řádků. Abstrakt max.
na 10 řádků. Abstrakt max. na 10 řádků. Abstrakt max. na 10 řádků. 

\bigskip{}


\noindent \emph{Klíčová slova:} klíčová slova (nebo výrazy) seřazená
podle abecedy a oddělená čárkou

\vfill{}
~

\selectlanguage{american}%
\begin{onehalfspace}
\noindent \emph{Title:}

\noindent \textbf{Data mining on the COMPASS tokamak experiments}
\end{onehalfspace}

\bigskip{}


\noindent \emph{Author:} Matěj Zorek

\bigskip{}


\noindent \emph{Abstract:} Max. 10 lines of English abstract text.
Max. 10 lines of English abstract text. Max. 10 lines of English abstract
text. Max. 10 lines of English abstract text. Max. 10 lines of English
abstract text. Max. 10 lines of English abstract text. Max. 10 lines
of English abstract text. Max. 10 lines of English abstract text.
Max. 10 lines of English abstract text. Max. 10 lines of English abstract
text. Max. 10 lines of English abstract text. Max. 10 lines of English
abstract text. Max. 10 lines of English abstract text. Max. 10 lines
of English abstract text. Max. 10 lines of English abstract text.
Max. 10 lines of English abstract text. Max. 10 lines of English abstract
text. Max. 10 lines of English abstract text. Max. 10 lines of English
abstract text. Max. 10 lines of English abstract text. Max. 10 lines
of English abstract text. Max. 10 lines of English abstract text.
Max. 10 lines of English abstract text. Max. 10 lines of English abstract
text. Max. 10 lines of English abstract text.

\bigskip{}


\noindent \emph{Key words:} keywords in alphabetical order separated
by commas

\selectlanguage{czech}%
\newpage{}

~\newpage{}

\pagestyle{plain}

\tableofcontents{}

\newpage{}


\chapter*{Úvod}

\addcontentsline{toc}{chapter}{Úvod}
V dnešní moderní společnosti funguje téměř vše na elektřinu a nároky stále rostou. Řešením by mohlo být ovládnutí termojaderné reakce. Abychom tento potenciál mohly naplno využít, potřebujeme tuto reakci pořádně pochopit. Jedním z nepokročilejších zařízeníxh sloužících k jejímu výzkumu jsou tokamaky. Znalost okamžitého stavu plazmatu uvnitř tokamaku je tedy poměrně důležitá. Právě tímto problémem se bude tato práce zabývat.

Cílem práce je nalézt způsob, jak určit tyto stavy v reálném čase za použití strojového učení. Nejdříve je potřeba seznámit se se základní fyzikální podstatou jevů, vyvstávajících při výbojích na tokamaku. 
Následně je třeba prostudovat metody strojového učení schopné řešit tento problém. Poté budu tyto algoritmy natrénovány na skutečných datech z tokamaku COMPASS a aplikovány na testovací vzorek dat. Posledním úkolem je vyhodnotit výsledky a porovnat metody mezi sebou.

V první kapitole je stručně zodpovězeno, co je to plazma. Následuje popis základní problematiky tokamaků. Dále je zde podrobněji představen problém, který tato práce řeší. Na konci kapitoly je vysvětleno strojové učení.

V druhé kapitole jsou uvedeny dvě metody strojového učení. Nejprve je podrobně popsán Skrytý Markovův model. Poté je vysvětlen Viterbiho algoritmus sloužící právě při výpočtu dříve jmenovaného modelu. Na konci této kapitoly představena clusteringová metoda K-means.

Ve třetí a poslední kapitole jsou postupně popsány rysy používané při aplikaci obou metod. Následuje seznam metrik, sloužících k vyhodnocení úspěšnosti výsledků. Dále je zde uveden postup práce a řešení dílčích problémů. Tato kapitola je zakončena porovnáním výsledků použitých metod.


\chapter{Úvodní terminologie}
\pagestyle{headings}

V této kapitole si nejdříve zodpovíme, co je to plazma. Dále se seznámíme se základní problematikou tokamaků, zejména tokamaku COMPASS. Poté si podrobněji představíme problém, kterým se bude tato práce zabývat. Nakonec bude uvedeno strojové učení a jeho dvě hlavní skupiny.

%V posledních desetiletích se rozšiřuje fenomén čisté fúzní energie, jež bude zísána slučováním.
%\section{"Co a proč to dělám"}

\section{Plazma}
Plazma je jedním ze čtyř základních skupenství. Jedná se o ionizovaný plyn, jehož atomy jsou rozděleny na pozitivní ionty a naegativní elektrony. %Je tvořená ionizovaným plynem s atomy rozděleným na pozitivní ionty a negativné elektrony. 
Na rozdíl od zbylých tří skupenství se volně, za normálních podmínek, na zemském povrchu nevyskytuje. Paradoxně však $99\% $ veškeré hmoty ve vesmíru tvoří právě plazma. 

V laboratoři se získává typicky zahříváním a ionizováním malého množství plynu pomocí elektrického proudu nebo radiových vln. Obykle tyto prostředky dodávají energii přímo volným elektronům uvnitř. Poté se během srážek těchto nabytých elektronů s atomy uvolňují další elektrony a tento kaskádový proces postupuje, dokud plyn nedosáhne požadovaného stupně ionizace.

Rozdíl mezi velmi slabě ionizovaným plynem a plazmatem je do značné míry záležitostí terminologie a způsobu interpretace. V závislosti na okolní teplotě prostředí a hustotě, dělíme plazmata na částečně ionizované a plně ionizované. Příkladem částečně ionizovaného plazmatu je třeba blesk nebo neonové světlo. Naproti tomu plně ionizované plazma lze nalézt uvnitř slunce nebo právě v tokamaku. 

\section{Tokamak COMPASS}
Název tokamak je zkratkou půvoního ruského názvu toroidalnaja kamera s magnitnymi katuškami (toroidní komora s magnetickými cívkami). V podstatě se jedná o experimentální zařízení využívné k tvorbě vysokoteplotního plazmatu a jeho následné kontrole pomocí magnetického pole. V současnosti jsou tokamaky považovány za jednu z nejnadějnějších cest k dosažení kontrolované jaderné fůze. Pokud by se podařilo fůzy efektivně a trvale udržet, mohlo by se pak přebytečné teplo převést, po vzoru tepelných elektráren, na elektřinu. Tímto způsobem bychom získali téměř nevyčerpatelný zdroj energie, který by byl současně šetrný k životnímu prostředí.

Na rozdíl od jaderných reaktorů, kde probíhá štěpení těžkých jader uranu $^{235}U$ na lehčí jádra, v tokamacích dochází ke slučování lehkých jader za účelem vzniku těžších jader. Při této termojaderné reakci se nejčastěji slučují jádra deuteria a tricia. Výsledkem reakce je hélium a nositel energie neutron.
Problém však tkví v tom, že pro udržení termojaderné fůze je zapotřebí velmi vysokých teplot a dostatečné doby trvání. Abychom toho docílili, musíme držet částice uprostřed toroidní komory, protože při vychýlení nebo kontaktu se stěnou, dochází k velice rychlému ochlazování. Proto se využívá silné magnetické pole, produkované cívkami, k manipulaci s nabytými částicemi plazmatu. 

Tokamak COMPASS je umístěn v Praze Ládví na Ústavu fyziky plazmatu Akademie věd České republiky již od roku 2004. %Je hlavním experimentálním zařízením tamějšího oddělení tokamaků. 
Původně byl umístěn a provozován ve Velké Británii pod UKAEA (UK Atomic Energy Autority) do roku 2002, kdy byl nahrazen tokamakem MAST. Díky svým rozměrům je řazen do kategorie menších tokamaků. I přes svou malou velikost umožňuje dosáhnout vysokoudržitelného stavu plazmatu neboli H-módu (High-confinement mode) %. Zároveň však 
a zároveň odpovídá desetině velikosti tokuamaku ITER, v současnosti budovanému ve Francii. Právě díky těmto dvěma vlasnostem je nyní využíván ke studiu specifických jevů, které jsou třeba k pochopení plazmatu a jeho následného udržení v rovnováze.

\begin{figure} [H] 
	\renewcommand{\figurename}{Obr.}
	\centering
	\includegraphics[scale=0.5]{Obr/compass1}
	\caption{Na levém obrázku je vyobrazen průřez tokamakem COMPASS a na pravém obrázku je jeho toroidní vakuová komora.  \cite{comp_foto1}}
	\label{obr_comp1}
\end{figure}


\section{Podrobnější popis problematiky}
V předchozí podkapitole bylo zmíněno, že tokamak COMPASS slouží v současnosti ke studiu chování plazmatu a jeho udržení.
Plazma se může uvnitř tokamaku nacházet ve čtyřech základních stavech. 
Prvním z nich je nízkoudržitelný L-mód (Low-confinement mode). V tomto stavu se plazma nachází bezprostředně po zažehnutí nebo případně po skončení H-módu. Pokud se plazma nachází v L-módu, je velice náročné udržet reakci na delší dobu a téměř nemožné udržet jí dlouhodoběji. 

Druhým stavem je již dříve zmínění H-mód. V toto stavu je možné lépe kontrolovat chování plazmy a především jí držet v rovnováze po delší dobu. 
Tento stav je také standartním referenčním režimem budoucího tokamaku ITER. 

Třetím v řadě je ELM (edge-localized mode). ELMy jsou v podstatě narušující nestability, k nímž dochází na okraji plazmy. ELMy se navíc mohou vyskytovat pouze během H-módu.
Posledním stavem je disrupce. 
Disrupce představuje fyzikální jev, během něhož dochází k přetržení nebo náhlé zrátě udžení plazmatu.

Aby bylo možné správně porozumět těmto jevům je zapotřebí velkého množství informací. Příkladem může být teplota během jednotlivých stavů atd. Bohužel většina detektorů a měřících přístrojů je limitovaná svou snímkovací frekvencí. Proto by bylo třeba vědět, v jakém stavu se zrovna plazma nachází, aby bylo možno měřit ve správnou chvíli. 

V této práci se budeme snažit najít způsob/y, jak klasifikovat první tři výše zmíněné stavy plazmatu v reálném čase. Využívat k tomu budeme data naměřených hodnot záření $H_\alpha$\footnote[1]{$H_\alpha$ je specifická červená spektrální čára vyzařovaná vodíkem s vlnovou délkou $656,28 nm$} a metody strojového učení. 
Náhled ideálně vypadajícího signálu je možné vidět na Obr. \ref{example1}. Na tomto obrázku je možné přesně sledovat všechny tři stavy. Signál začíná v L-módu. Asi po $10$ milisekundách přechází do H-módu. Jelikož H-mód  je lépe udržitelný, nevyzařuje plazma tolik vodíku jako v L-módu. Jednotlivé peaky jsou pak ELMy. Ve vákuu, jako je uvnitř tokamaku, se všechny částice rozprostřou po stěnách komory. Když se pak vlivem nějakých nestabilit utrhne kus plazmatu a dotkne se stěny, částice, jež se na stěně nacházejí, se rozzáří a my vidíme pěkný ELM.
V praxi však signály vypadají spíše jako na Obr. \ref{example2}, kde už některé stavy nejsou jednoznačně viditelné.
\begin{figure} [H] 
	\renewcommand{\figurename}{Obr.}
	\centering
	\includegraphics[scale = 0.6]{Obr/ideal}
	\caption{Ukázka ideálně vypadajícího záření $H_\alpha$ zaznamenaného detektrorem. (číslo výstřelu: 15364)}
	%Ukázka tří základních stavů plazmatu v tokamaku na naměřených datech z detektrou zaznamenávajícho záření $H_\alpha$. 
	\label{example1}
\end{figure}

\section{Strojové učení}
Strojové učení se zabývá počítačovými technikami a algoritmy, často za pomoci statistických metod, dávajícími počítačům schopnost se učit.  Schopnost učit se, nelze v tomto kontextu brát úplně doslovně, spíše je to schopnost postupně zlepšovat svou výkonnost či přesnost při řešení specifického problému. % bez potřeby expicitního programování. 
Strojové učení je velmi úzce spjato s oblastmi výpočetní statistiky a matematickou optimalizací. Zatímco první z nich se zaměřnuje na tvorbu předpovědí nebo rozhodování s pomocí počítačů, druhá oblast poskytuje teorii, metody a v neposlední řadě aplikaci. 

Strojové učení dělíme do dvou hlavních skupin. První z nich je známo jako učení bez učitele (unsupervised learning), které úzce souvisí s těžbou dat. Vyznačuje se tím, že algoritmus nebo metoda pracuje zásadně s neoznačenými daty tzn. neznáme výsledky. 

Druhou skupinou je učení s učitelem (supervised learning) lišící se předběžnou znalostí rozdělení dat. Tuto dodatečnou znalost lze pak využít k přesnějším a kvalitnějším výsledkům. Obě tyto skupiny mají svá jedinečná uplatnění, jako jsou například: regrese, klasifikace, clustering atd.


\begin{figure} [H] 
	\renewcommand{\figurename}{Obr.}
	\centering
	\includegraphics[scale=0.5]{Obr/hruza}
	\includegraphics[scale=0.5]{Obr/hruza1}
	\caption{Záření $H_\alpha$ častější případy. (horní obrázek  č.v.: 7880, spodní obrázek č.v.: 13484)}
	\label{example2}
\end{figure}

%a na základě jejich vlastností, podobností nebo nějakého vzoru je následně rozdělí do skupi
\subsection{Klasifikace vs clustering}
Klasifikace a clustering jsou dvě strany jedné mince. Obě metody slouží k rozdělování pozorovaných dat do různých skupin. Toto členění probíhá na základě analýzy vlastností, podobností nebo nějakého skrytého vzoru.
Zatímco klasifikace je jednoznačnou ukázkou učení s učitelem, při kterém algoritmus analyzuje jednotlivé jedince každé skupiny, aby odhalil, proč jsou právě v dané skupině. Clustering je naopak příkladem učení bez učitele, při kterém jsou zkoumána všechna data za účelem nalezení vztahů mezi některými z nich. Pokud jsou nějaké takové vztahy nalezeny, jsou pak tyto data rozdělena do příslušných clusterů (shluků).

\chapter{Teoretická část}
\pagestyle{headings}
V této kapitole se budeme zabývat teorií ohledně dvou námi použitých metod strojového učení. Nejprve se seznámíme se základními principy Skrytého Markovova modelu. Poté si přiblížíme Viterbiho algoritmus, který slouží k výpočtům již zmíněného modelu a nakonec si představíme ryze clusteringovou metodu K-means.

\section{Skrytý Markovův model}

Skrytý Markovův model známý spíše pod svým anglickým názvem "Hidden Markov model" $ \ $je statistický model, který slouží k modelování Markovských procesů se skrytýmy stavy.
Přesněji se jedná o dvojnásobně zapuštěný stochastický proces podložený dalším stochastickým procesem, který není pozorován, je tedy skrytý. Nicméně tento skrytý proces může být pozorován skrze jiné stochastické procesy, jež poskytují posloupnost pozorování. 

Skrytý Markovův model je široce používán v rozpoznávání řeči (speech recognition), modelování přirozeného jazyka, rozpoznávání ručně psaného písma a analýza biologických sekvencí, jako například DNA a proteinů.

%Pro lepší představu uvažujme systém uren a míčků vyobrazený na  "obrázek".
Pro lepší představu si tento model ukážeme na příkladě uren a míčků. Předpokládejme, že v místnosti je $N$ velkých skleněných uren. V každé urně je velký počet barevných míčků. Předpokládejme, že máme $M$ odlišných barev míčků. Fyzikální proces pro získání pozorování je následující. Džin je v místnosti a on (nebo ona) podle nějakého náhodného procesu vybírá počáteční urnu. Z této urny vybere náhodně míček a jeho barva je nahrána jako pozorování. Míček je pak nahrazen v urně, z níž byl vybrán. Další urna je vybrána procesem náhodného výběru spojeného se současnou urnou a výběr míčku je opakován. Celý proces generuje konečný počet pozorování posloupnosti barev, který bychom rádi modelovali jako pozorovaný výstup skrytého markovského modelu. (příklad převzat z \cite{Rabiner})

Dále se na tento model můžeme dívat
%Na tento model se můžeme také dívat
 jako na specifický případ stavového prostorového modelu, ve kterém jsou skryté proměnné diskrétní. 
Nicméně, když prozkoumáme jednorázový řez modelu, vidíme že odpovídá Mixture distribution (viz kapitola $9$ v \cite{Bishop}) s hustotou pravděpodobnosti danou  $\mathbb{P}(\vec{x}|\vec{z})$.
Proto ho můžeme interpretovat jako rozšíření Mixture modelu, kde výběr složky směsi, pro každé pozorování, není nezávislý, ale závisí na volbě složek z předchozího pozorování.

Jako v případě standartního Mixture modelu, skryté proměnné jsou diskrétní multinomické proměnné $\vec{z}_n$ popisující složku směsi, jež je zodpovědná za generování příslušného pozorování $\vec{x}_n$. 
Od této chvíle budeme předpokládat, že skrytý proces je Markovův, tzn. splňuje Markovu vlastnost (Markov property)\cite{Markov Chain}. Z tohoto předpokladu nyní plyne, že budoucí stav skryté proměnné $\vec{z}_n$ závisí pouze na předchozím stavu $\vec{z}_{n-1}$ skrze podmíněnou pravděpodobnost $\mathbb{P}(\vec{z}_n|\vec{z}_{n-1})$.  Pak zavedeme matici přechodů $\mathbb{A}$ danou předpisem  $A_{i,j} \equiv \mathbb{P}(\vec{z}_{n,j}|\vec{z}_{n-1,i} = 1)$, navíc matice splňuje, že $A_{i,j} \in (0,1)$ a skoupce jsou normovány na $1$, tzn $\sum_j A_{i,j} \ =1$. Dále můžeme tedy explicitně napsat podmíněné rozdělení pro $K$ skrytých stavů ve tvaru

\begin{equation}
\mathbb{P}(\vec{z}_n|\vec{z}_{n-1},\mathbb{A}) = \prod_{i=1}^{K} \prod_{j=1}^{K}
A^{\vec{z}_{n-1,i} \vec{z}_n,j}_{i,j}.
\label{podmíněná pravděpodobnost explicitní}
\end{equation}


\begin{figure} [H] 
	\renewcommand{\figurename}{Obr.}
	\centering
	\includegraphics[scale=0.3]{Obr/obr4}
	\caption{Diagram přechodů, kde skryté proměnné mají tři možné stavy odpovídající třem boxům. Šipky označují prvky matice přechodů $A_{i,j}$. \cite{Bishop}}
	\label{tří stavý diagram}
\end{figure}

Tímto vzorcem můžeme vyjádřit všechny skryté stavy až na počáteční $\vec{z}_1$. Tento stav má pouze marginální rozdělení $\mathbb{P}(\vec{z}_1)$ reprezentované vektorem pravděpodobností $\pi$, který má tvar $\pi \equiv \mathbb{P}(\vec{z}_{1,j} = 1)$ a tedy 

\begin{equation}
	\mathbb{P}(\vec{z}_1|\pi) = \prod_{j=1}^{K} \pi^{\vec{z}_{1,j}}_j,
	\label{marginální pravděpodobnost}
\end{equation} 
kde $\sum_j \pi_j = 1$.
Kdybychom chtěli matici $\mathbb{A}$ vysvětlit i jinak, mohli bychom toho docílit graficky pomocí diagramu na Obr. \ref{tří stavý diagram}. Když tento diagram dále rozvineme s průběhem času, získáme takzvaný mřížový diagram, který nám poskytuje alternativní reprezentaci přechodů mezi jednotlivými skrytými stavy. Pro případ Skrytého Markovova modelu tento diagram nabývá tvaru Obr. \ref{HMM diagram}. 

Abychom měli pravděpodobnostní model kompletní, je třeba ještě zavést %podmíněné rozdělení pozorovaných proměnných
emisní pravděpodobnosti (emission probabilities) $\mathbb{P}(\vec{x}_n|\vec{z}_n, \Theta)$, kde $\Theta$ představuje soubor parametrů řídícího rozdělení. 
%Tyto pravděpodobnosti jsou známé jako emisní pravděpodobnosti (emission probabilities). 
Pro pozorované proměnné $\vec{x}_n$ se 
%Vzhledem k tomu, že $\vec{x}_n$ jsou pozorované,  tak se 
rozdělení $\mathbb{P}(\vec{x}_n|\vec{z}_n, \Theta)$ skládá z $K$-dimenzionálního vekoru odpovídajícího $K$ potenciálním stavům $\vec{z}_n$. %, pro dané hodnoty $\Theta$. 
Emisní pravděpodobnosti můžeme pak zapsat jako 

\begin{equation}
\mathbb{P}(\vec{x}_n|\vec{z}_n,\Theta) = \prod_{j=1}^{K} \mathbb{P}(\vec{x}_n|\Theta_j) ^{\vec{z}_n,j},
\end{equation}

přičemž mohou mít například tvar Gaussova ($R$-rozměrného) rozdělení
\begin{equation}
\mathbb{P}(\vec{x}_n|\vec{z}_n) = \prod_{j=1}^{K} \mathcal{N}(\vec{x}_n|\vec{\mu}_j,\Sigma_j) ^{\vec{z}_{n,j}} = \prod_{j=1}^{K}
\left( \frac{1}{(2\pi)^{R/2}}\frac{1}{|\Sigma_j|^{1/2}} exp \Bigg\{ -\frac{1}{2}(\vec{x}_n - \vec{\mu}_j)^T \Sigma_j^{-1}(\vec{x}_n - \vec{\mu}_j) \Bigg\} \right)^{\vec{z}_{n,j}}
\end{equation}
pokud prvky $\vec{x}_n$ jsou spojité.

Nyní se zaměříme na homogenní modely, pro které všechna podmíněná rozdělení řídící skryté proměnné sdílí stejné parametry $\mathbb{A}$ a podobně všechna emisní rozdělení sdílí stejné parametry $\Theta$.

Sdružené pravděpodobnostní rozdělení přes skryté i pozorované proměnné jsou pak dány vzorcem

\begin{equation}
\mathbb{P}(\vec{X},\vec{Z}|\widetilde{\vec{\Theta}}) = \mathbb{P}(\vec{z}_1|\pi) \prod_{n=2}^{N} \mathbb{P}(\vec{z}_n|\vec{z}_{n-1},\mathbb{A}) \prod_{m =1 }^{N} \mathbb{P}(\vec{x}_m|\vec{z}_m,\Theta),
\end{equation}
kde $\vec{X} = (\vec{x}_1,...,\vec{x}_N)$, $\vec{Z} = (\vec{z}_1,...,\vec{z}_N)$ a $\widetilde{\vec{\Theta}} =(\pi, \mathbb{A},\Theta)$ jsou parametry řídícího modelu.

\begin{figure} [H] 
	\renewcommand{\figurename}{Obr.}
	\centering
	\includegraphics[scale=0.3]{Obr/obr5}
	\caption{Stavový diagram Obr. \ref{tří stavý diagram} rozvinutý s časem. Každý sloupec diagramu odpovídá jedné skryté proměnné $\vec{z}_n$. \cite{Bishop}}
	\label{HMM diagram}
\end{figure}

\subsection{Viterbiho algoritmus}

Tento algoritmus navrhl Andrew Viterbi, již v roce 1967, za účelem dekódování konvolučních kódů, jež se používají nejen v mobilních sítích, ale také ke komunikaci se satelity a sondami ve vesmíru. V současnosti se používá k rozpoznávání a syntéze řeči, vyhledávání klíčových slov, v bioinformatice nebo, což je pro nás nejdůležitější, k hledání nejpravděpodobnějších posloupností stavů. 

V nejobecnější podobě se na Viterbiho alogritmus můžeme dívat jako na řešení problému maximálního aposteriorního pravděpodobnostního odhadu posloupnosti skrytých stavů konečného diskrétního Markova procesu. Tento problém je formálně identický s problémem hledání nejkratší cesty grafem. Posloupnost pozorování $\vec{X}$ každé cesty může byt určena jako délka úměrná $-\ln{\mathbb{P}(\vec{Z},\vec{X})}$, kde $\vec{Z}$ je seqvence stavů spojená s příslušnou cestou. Tento poznatek nám dovoluje řešit problém hledání posloupnosti stavů, pro které je $\mathbb{P}(\vec{Z},\vec{X}) = \mathbb{P}(\vec{Z}|\vec{X}) \mathbb{P}(\vec{X})$ maximální, jako problém hledání cesty jejíž délka $-\ln{\mathbb{P}(\vec{Z},\vec{X})}$ je minimální. Poněvadž $\ln{\mathbb{P}(\vec{Z},\vec{X})}$ je monotonní funkcí $\mathbb{P}(\vec{Z},\vec{X})$ a každá cesta odpovídá právě jedné posloupnosti stavů. Následně díky platnosti Markovy vlastnosti můžeme přepsat $\mathbb{P}(\vec{Z},\vec{X})$ jako 

\begin{equation}
\mathbb{P}(\vec{Z},\vec{X})  = \mathbb{P}(\vec{X}|\vec{Z}) \mathbb{P}(\vec{Z}) = \mathbb{P}(\vec{z}_1) \left[\prod_{n = 2}^{N} \mathbb{P}(\vec{z}_{n}|\vec{z}_{n-1}) \right] \prod_{n=1}^{N} \mathbb{P}(\vec{x}_n|\vec{z}_n).
\label{produkt viterbi}
\end{equation}

Po zlogaritmování \eqref{produkt viterbi} můžeme vidět, že celková délka cesty odpovídající libovolnému $\vec{Z}$ je 

\begin{equation}
-\ln{\mathbb{P}(\vec{Z},\vec{X})} = \left[\sum_{n=2}^{N} -\ln{\mathbb{P}(\vec{z}_{n}|\vec{z}_{n-1})} - \ln{\mathbb{P}(\vec{x}_n|\vec{z}_n)}\right]  -\ln{\mathbb{P}(\vec{z}_1)} - \ln{\mathbb{P}(\vec{x}_1|\vec{z}_1)}, 
\label{ln cesta}
\end{equation}
kde $-\ln{\mathbb{P}(\vec{z}_{n}|\vec{z}_{n-1})} - \ln{\mathbb{P}(\vec{x}_n|\vec{z}_n)}$ je délka přechodu ze $\vec{z}_{n-1}$ do $\vec{z}_n$.


\begin{figure} [H] 
	\renewcommand{\figurename}{Obr.}
	\centering
	\includegraphics[scale=0.6]{Obr/Vi}
	\caption{(a) Mřížový diagram s délkami přechodů. M = 4, K = 5. (b) Rekurzivní hledání nejkratší cesty pomocí Viterbiho algoritmu.}
	\label{Viterbi}
\end{figure}

Ve chvíli, kdy jsme našli nejpravděpodobnější cestu, a tím pádem i združené rozdělení $\mathbb{P}(\vec{Z}, \vec{X})$, potřebujeme už jen nalézt posloupnost stavů odpovídající této cestě pomocí rekurze.

Jednou z největších předností Viterbiho algoritmu je jeho efektivita. Jelikož počet možných cest roste exponenciálně s délkou řetězce, je tak pro většinu algoritmů výpočetně velice náročný a v některých případech i nemožný. To ovšem neplatí pro tento algoritmus, neboť výpočetní náročnost Viterbiho algoritmu roste pouze lineárně s délkou řetězce.

\section{K-means Clustering}
K-means clustering je typ strojového učení bez učitele, který se používá v případě, že chceme zpracovat neoznačená data, tzn. nemáme předem definované skupiny či kategorie. Právě hlavním cílem algoritmu je najít tyto skupiny.

Předpokládejme, že máme $N$ pozorování $(\vec{x}_1, ... ,\vec{x}_N)$ náhodné $R$ rozměrné veličiny $\vec{X}$ a chceme je rozdělit do nějákých $K$ shluků.  Pod pojmem shluk budeme rozumět skupinu bodů, jejichž vzájemné vzdálenosti jsou mnohem menší v porovnání se vzdálenostmi k bodům vně skupiny. Dále je pak potřeba zavést si vektory $\vec{\phi}_j$, kde $j \in 1,..,K$. Tyto vektory představují středy našich shluků a naším cílem se nyní stává nalezení množiny $\{\vec{\phi}_j \}$, tak aby bylo splněno, že součet čtverců vzdáleností každého bodu shluku k nejbližšímu vektoru $\vec{\phi}_j$ je minimální. Jinými slovy, potřebujeme minimalizovat objektovou funkci $\rho$ definovanou jako
\begin{equation}
\rho = \sum_{n=1}^{N}\sum_{j=1}^{K} b_{n,j} \ ||\vec{x}_n - \vec{\phi}_j \ ||^2,
\label{J}
\end{equation}
kde proměnné $b_{n,j} \in \{0,1\}$ příslušící každému bodu $\vec{x}_n$ indikují, zda tento bod patří $j$-tého shluku nebo nikoli. Pokud ano, $b_{n,j}$ je rovno $1$, v opačném případě nabývá $b_{n,j}$ hodnoty $0$. 
Minimálního $\rho$ lze dosáhnout pomocí dvoufázového iteračního procesu. Nejprve vybereme, nejlépe náhodně, počáteční vektory $\vec{\phi}_j$. V první fázy bereme $\vec{\phi}_j$ jako fixní a minimalizovat budeme s ohledem na $b_{n,j}$. Jelikož $\rho$ je vůči $b_{n,j}$ lineární a pro rozdílná $n$ jsou $b_{n,j}$ nezávislé, můžeme je minimalizovat pro každé $n$ zvlášť. Jednoduše bereme $b_{n,j}$ následně
\begin{equation}
 b_{n,j}  = \left\{ \begin{array}{ll}
1 &\mbox{pokud $argmin_m ||\vec{x}_n - \vec{\phi}_m||^2 = j$}\\
0 & \mbox{jinde}.\end{array} \right. 
\end{equation}
V druhé fázi je naopak $b_{n,j}$ pevné a minimalizujeme s ohledem na $\vec{\phi}_j$. Proto zderivujeme funkci $\rho$ podle $\vec{\phi}_j$ a tuto derivací položíme rovnu $0$, tzn.
\begin{equation}
2\sum_{n=1}^{N} b_{n,j}(\vec{x}_n - \vec{\phi}_j) = 0.
\label{minrho}
\end{equation}
Načež po osamostatnění $\vec{\phi}_j$. získáváme konečný tvar
\begin{equation}
\vec{\phi}_j = \frac{\sum_{n=1}^{N} b_{n,j} \vec{x}_n}{\sum_{n=1}^{N} b_{n,j}},
\label{k-mean}
\end{equation}
kde jmenovatel představuje celkový počet bodů patřících do $j$-tého shluku.
Vzorec \eqref{k-mean} lze ovšem interpretovat teké jako střední hodnotu (anglicky: mean) všech bodů příslušících do shluku $j$, odtud plyne i název "K-means".

\begin{figure} [H] 
	\renewcommand{\figurename}{Obr.}
	\centering
	\includegraphics[scale = 1.1]{Obr/kmeansViz}
	\caption{Vizualizace algoritmu K-means. Tréninková data jsou vyobrazena jako tečky a středy shluků jako křížky. Na obrázku $(a)$ jsou vykreslena původní data. V $(b)$ je možné vidět náhodně vybrané počáteční středy a $(c)-(d)$ ilustrují úvodní dvě iterace algoritmu. \cite{kmeansviz}}
	\label{K-means}
\end{figure}




\begin{comment}

 K-means algorithm. Training examples are shown as dots, and cluster centroids are shown as crosses. (a) Original dataset. (b) Random initial cluster centroids. (c-f) Illustration of running two iterations of k-means. In each iteration, we assign each training example to the closest cluster centroid (shown by "painting" the training examples the same color as the cluster centroid to which is assigned); then we move each cluster centroid to the mean of the points assigned to it.

K-means clustering is a type of unsupervised learning, which is used when you have unlabeled data (i.e., data without defined categories or groups). The goal of this algorithm is to find groups in the data, with the number of groups represented by the variable K. The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided. Data points are clustered based on feature similarity. The results of the K-means clustering algorithm are:

The centroids of the K clusters, which can be used to label new data
Labels for the training data (each data point is assigned to a single cluster)
Rather than defining groups before looking at the data, clustering allows you to find and analyze the groups that have formed organically. The "Choosing K" section below describes how the number of groups can be determined.  

Each centroid of a cluster is a collection of feature values which define the resulting groups. Examining the centroid feature weights can be used to qualitatively interpret what kind of group each cluster represents.  

This introduction to the K-means clustering algorithm covers:

Common business cases where K-means is used
The steps involved in running the algorithm
A Python example using delivery fleet data
\end{comment}



%\section{kNN}


\chapter{Postup práce a výsledky}
\pagestyle{headings}

\section{Rysy (Features)}

Ve strojovém učení a rozpoznávání vzorů se pod pojmem "rys" \ (feature) rozumí individuální měřitelná vlastnost nebo charakteristika pozorovaného jevu. Výběr těchto rysů je %jedním z nejdůležitějších kroků v řešení daného problému a  
naprosto zásadní pro efektivní rozpoznávací, regresní a klasifikační algoritmy. 
Čím relevantnější  a charakterističtější rys, tím lépe jsme schopni docílít větší přesnosti modelu. Na druhou stranu vynechání zbytečných, případně méně důležitých rysů zase snižuje složitost modelu a urychluje jeho trénink.
Nejčastější forma rysu je číselná hodnota, avšak při rozpoznávání syntetických vzorků se hojně používají i písmena, slova nebo grafy.

Selekci těchto rysů je možné demonstrovat na následujícím příkladu. Předpokládejme, že bychom chtěli předvídat typ domácího mazlíčka, jež si někdo koupí.

Do rysů můžeme zahrnout například věk osoby, pohlaví, jméno, bydlení (byt, dům, ...), rodinný příjem, vzdělání a počet dětí. Je zřejmě, že většina těchto rysů nám může při předvidání pomoci, ale některé jako třeba vzdělání nebo jméno jsou zjevně méně důležité.%nedůležité. 

\begin{table}[H]
	\centering
	%\renewcommand{\arraystretch}{1}
	%\renewcommand{\tablename}{Tab.}
	
	\begin{tabular}{c|c|c|c|c|c|c}
		
		jméno & věk &	pohlaví	&	bydlení &  příjem & počet dětí & vzdělání	\\
		\hline
		Karel & 25 & muž & byt & 30.000 & 0 & středoškolské \\
		\hline
		Petr & 30 & muž & dům & 45.000 & 2 & vysokoškolské \\
		\hline
		Jana & 42 & žena & byt & 23.000 & 1 & základní \\
		\hline
		Miloš & 51 & muž & dům & 29.000 & 1 & středoškolské\\
		\hline
		\multicolumn{7}{c}{...}  \\
	\end{tabular}
	\caption{Vzorová tabulka rysů k demonstračnímu příkladu}
\end{table}


\begin{comment}

Například pokud se snažíte předvídat typ zvířete, který si někdo zvolí, vaše vstupní funkce mohou zahrnovat věk, domácí region, rodinný příjem atd. Označení je konečnou volbou, jako je pes, ryba, iguana, rock, atd.

ve strojovém učení a rozpoznávání vzoru je rysem individuální měřitelná vlastnost nebo charakteristika pozorovaného jevu. [1] Výběr informačních, diskriminačních a nezávislých prvků je zásadním krokem pro efektivní algoritmy v rozpoznávání, klasifikaci a regresi. 

Rysy jsou nejčastěji číselné avšak při rozpoznávání syntetických vzorů se navíc používají například řetězce a grafy.  

Features are a column of data given as the input. They are also called as attributes or might sometimes be referred as dimensions.

A particular problem data set can have several features tagging to them. It is important to select the features that are more relevant to our problem so that the accuracy of the model improves. It also reduces the complexity of the model as we avoid the least significant / unnecessary feature data. The simpler model is simpler to understand and explain.

This Process is called feature engineering / selection and is one of the crucial step of pre-processing. Different algorithms can be used to implement it.

The Features can be of different types.

Simple Supervised selection where they are simple values like numbers and characters.
Eg: Size of the house (number) .

In unsupervised learning, the model is itself trained to recognize the features and work on it.

Eg: In character recognition, features may include histograms counting the number of black pixels along horizontal and vertical directions, number of internal holes, stroke detection and many others.
\end{comment}

\subsection{První a druhá derivace}
Prvním rysem použitým při  klasifikaci je první derivace.
Jelikož jsou k dispozici pouze jednotlivé body, nelze použít analytický vzorec pro derivace funkce $f(x):R \longrightarrow R$, tedy konkrétně
\begin{equation}
\frac{d f(x)}{dx}=f'(x) = \lim\limits_{h->0} \frac{f(x+h)-f(x)}{h}.
\label{derivace}
\end{equation}
Namísto toho se musí počítat numericky, a to za použití centrální diference druhého řádu pomocí \eqref{vnitřní diference} a v krajních bodech pomocí jednostranných diferencí prvního nebo druhého řádu \eqref{vnější diference}.
\begin{equation}
\hat{f}'_k = \frac{f(x_{k+1})-f(x_{k-1})}{2h} 
\label{vnitřní diference}
\end{equation}
\begin{equation}
\hat{f}'_0 = \frac{f(x_1)-f(x_0)}{h} \ \ \text{a} \ \ \hat{f}'_n = \frac{f(x_n)-f(x_{n-1})}{h}
\label{vnější diference}
\end{equation}

Dalším použitým rysem je druhá derivace, kterou lze je snáze získat použitím výše zmíněných vzorců \eqref{vnitřní diference} a \eqref{vnější diference} na již jednou zderivovaný signál.

\subsection{Savitzky-Golay filtr}
Na obrázku \ref{example2} v první kapitole bylo možné vidět, že naše data získaná z detektoru, jsou zatížena velikým šumem. Proto není od věci pokusit se tento signál nějak vyhladit. Za tímto učelem byl mezi rysy vybrán Savitzky-Golay filtr, jež je digitálním filtrem dobře přizpůsobeným pro vyhlazování dat. S-G filtry byly zpočátku použity k zobrazení relativních šířek a výšky spektrálních čar v zašuměných spektrometrických datech.

Digitální filtr aplikovaný na stejnoměrně rozložená data, tzn. $f_i = f(t_i)$, kde $t_i = t_0 \cdot \Delta$,  $i \in \mathbb{Z}$ a $\Delta$ je konstanta, nahrazuje každou hodotu $f_i$ lineární kombinací $g_i$ nebo sama sebe a určitým počtem nejbližších sousedů,
\begin{equation}
g_i = \sum_{n = -n_L}^{n_R} c_n f_{i+n} = (\vec{c}\otimes \vec{f})_j,
\label{Digital filter}
\end{equation} 
kde $\vec{c}$ jsou tzv. konvoluční koeficienty, $n_L$ je počet použitých bodů vlevo a $n_R$ vpravo. %Konkrétně S-G filtr používá konfiguraci $n_L=n_R$.

Hlavní myšlenou S-G filtru je nalezení koeficientů $c_n$ tak, aby se zachovávaly momenty vyšších řádů a aproximace funkce uvnitř pohybujícího se okna pomocí polynomu namísto konstanty. Pro každou hodnotu $f_i$ proložíme všech $n_L+n_R+1$ bodů, uvnitř pohyblivého okna, polynomem pomocí metody nejmenších čtverců a nastavíme $g_i$ na hodnotu polynomu na i-té pozici. Jelikož nevyužíváme hodnoty polynomu v jiných bodech, měli bychom tedy pro $f_{i+1}$ udělat celou proceduru znovu. Naštěstí díky tomu, že metoda nejmenších čtverců pro výpočet využívá pouze lineární maticovou inverzi a koeficienty proloženého polynomu jsou sami o sobě také lineární. Můžeme vešeré prokládání vypočítat dopředu a pomocí binárního vektoru lze pak vše dopočítat linerání kombinací.

Potřebujeme tedy proložit polynom řádu $M$ kontrétně $a_0 + a_1 i + ... + a_M i^M$ hodnotami $f_{-n_L}, ..., f_{n_R}$. Matice pro nejmenší čtverec má tvar 
\begin{equation}
A_{ij} = i^j \ \ \ \ \ i = -n_L ,..., n_R \ \ \ a \ \ \ j = 0,...,M
\end{equation} 
koeficienty polynomu $\vec{a}$ lze pak vypočítat jako 
\begin{equation}
\vec{a} = (\mathbb{A}^T \cdot \mathbb{A})^{-1} \cdot (\mathbb{A}^{T} \cdot \vec{f}).
\end{equation}

Nakonec vypočítáme i koeficienty $\vec{c}$ pomocí
\begin{equation}
\vec{c} = (\mathbb{A}^T \cdot \mathbb{A})^{-1} \cdot \mathbb{A}^{T}
\end{equation}
a po dosazení zpět do \eqref{Digital filter} získáváme konečně vyhlazenou funkci $g_i$. 

S-G filtr je pro řešení našeho problému výhodný také z důvodu, že po mírné úpravě lze takto počítat i vyhlazené derivace  signálu.

\begin{figure} [H] 
	\renewcommand{\figurename}{Obr.}
	\centering
	\includegraphics[scale=0.65]{Obr/obr6}
	\includegraphics[scale=0.65]{Obr/obr7}
	\caption{Savitzky-Golay filtr. Na horním obrázku jsou syntetická data s bílým gaussovským šumem. Na spodním je již vyhlazený set získaný aplikací S-G filtru s parametry $n_L =16, n_R = 16$ a stupeň polynomu $M =4$. \cite{NumRec}}
	\label{S-G example}
\end{figure}

\subsection{Klouzavý průměr}
Dalším vybraným rysem je klouzavý průměr. Klouzavý průměr je diskrétní lineární filtr s konečnou dobou odezvy, který slouží k vyhlazení signálu. Nadále ho budeme značit jako $\widetilde{X}_t $. Nechť máme  vektor naměřených hodnot $\vec{X}=(x_1, x_2,...,x_n)$, pak definuji klouzavý průměr pro $t \ \in \ 1,2,...,n$ jako
\begin{equation}
\widetilde{X}_t = \frac{1}{\tilde{\omega}} \sum_{k = t-\tilde{\omega} }^{t} x_k , 
\label{klouzavý průměr}
\end{equation}  
kde $\tilde{ \omega } = min\{ \omega, t \} $, přičemž $\omega$ je délka úseku. Pak vektor $\vec{\widetilde{X}} = (\widetilde{X}_1,\widetilde{X}_2,..,\widetilde{X}_n)$ je rys vektoru $\vec{X}$.

Ve skutečnosti se jedná o standartní aritmetický průměr \eqref{aritmetický průměr}, jež je aplikovaný pouze na úsek dat konečné délky.
\begin{equation}
\overline{X}_n = \frac{1}{n} \sum_{k=1}^{n} x_k,
\label{aritmetický průměr}
\end{equation}

Mezi rysy byl vybrán, protože předpokládáme, že okamžitá hodnota je závislá na %bezprostředně 
předchozích datech. Důvod, proč využíváme jen konečně dlouhý úsek předcházejících hodnot je ten, že ze zákona velkých čísel aritmetický průměr konverguje ke střední hodnotě, a tedy ke konstantě. To znamená, že postupem času budou mít rozdílná data v odlišných stavech stejnou hodnotu rysu. Z čehož plyne, že takovýto rys by jen zkresloval a znepřesňoval výsledek, viz Obr. \ref{obr1}. 


\begin{figure} [H] 
	\renewcommand{\figurename}{Obr.}
	\centering
	\includegraphics[scale=0.7]{Obr/obr2}
	\caption{Rozdíl mezi klouzavým a aritmerickým průměrem aplikovaným na syntetická data}
	\label{obr1}
\end{figure}



\subsection{Exponenciální klouzavý průměr}
Již dříve jsme se zmínili, že předpokládáme závislost na předchozích hodnotách. Nicméně je zřejmé, 
%"Jak jsem se zmínil již dříve, předpoládám závislost na předchozích hodnotách. "Není však překvapením"
že hodnoty naměřené s velkým časovým rozestupem %vzdálenější hodnoty 
na sebe mají mnohem menší vliv než ty, jež jsou naměřeny bezprostředně za sebou. Proto dalším vybraným rysem je tedy exponenciální klouzavý průměr $S_t$. 

Nechť $\vec{X}=(x_1, x_2,...,x_n)$ je vektor naměřených hodnot, pak definuji váhový součet zleva pro $t \ \in \ 1,2,...,n$ jako
\begin{equation}
S_t = \frac{1}{\tilde{\omega}} \sum_{k=t-\tilde{\omega}}^{t} g_{t-k} \cdot x_k, 
\label{exponenciální klouzavý průměr}
\end{equation} 
kde $\tilde{ \omega } = min\{ \omega, t \} $, $\omega$ je délka úseku a $g_m$ je váhová funkce tvaru $g_m = \gamma^m$, přičemž $\gamma \in (0,1)$. Pak vektor $\vec{S} = (S_1,S_2,..,S_n)$ je rys vektoru $\vec{X}$. 
Díky exponenciální váhové funkci $g_m$, jsme tedy schopni snížit důležitost více vzdálených dat, což byl náš záměr.  

\begin{comment}

\subsection{Rozptyl}
Rozptyl asi vynechám protože je k ničemu
\begin{equation}
\sigma ^2_n = \frac{1}{n} \sum_{k = 1}^{n} (x_k -E(x) )^2 = \frac{1}{n} \sum_{k = 1}^{n} (x_k - \overline{X}_n )^2
\label{rozptyl}
\end{equation}
\end{comment}


\begin{comment}

Nechť $X = (x_1, x_2, ..., x_n)$ je diskrétní náhodná veličina s příslušnými pravděpodobnostmi  $p_1,p_2, ...,p_n$. Pak střední hodnotu náhodné veličiny $X$ označím symbolem $E[X]$ a definuji ji jako 
\begin{equation}
E[X]  = \sum_{k=1}^{n} p_k \cdot x_k.
\label{etření hodnota}
\end{equation}
Pokud jsou $x_1, x_2, ..., x_n$ stejně rozdělené tzn. $p_1 = p_2 = ... = p_n$ pak stření hodnota nabývá tvaru 
\begin{equation}
E[X] = \frac{1}{n} \sum_{k=1}^{n} x_k
\label{mean}
\end{equation}
\end{comment}

\subsection{Klouzavý rozptyl}
Ve statistice a teorii pravděpodobnosti se pod pojmem rozptyl rozumí střední hodnota kvadrátu odchylky od střední hodnoty náhodné veličiny. Bývá reprezentován symbolem $Var(X)$ nebo $\sigma^2$ a definován vzorcem 
\begin{equation}
Var(X) = E[(X-E[X])^2] %= \sum_{k=1}^{n}(x_k-E[X])^2
\end{equation}
pro stejně rozdělené diskrétní náhodné veličiny $\vec{X} = (x_1, x_2, ...,x_n)$
můžeme tento vzorec přepsat do tvaru 

\begin{equation}
Var(X) = \frac{1}{n}\sum_{k=1}^{n} (x_k - \overline{X}_n)^2.
\label{varf}
\end{equation}

Bohužel, rozptyl nemůžeme jako rys použít ze stejného důvodu jako aritmetický průměr, protože s postupem času bude různým stavům přiřazovat stejnou hodnotu. 
Proto zde využijeme místo aritmetického průměru již dříve definovaný klouzavý průměr a výslednou veličinu %jsem označil jako úsekový rozptyl. Úsekový rozptyl budu nadále značit jako $D_m$ a pro náhodné veličiny $X = (x_1, x_2, ...,x_n)$ ho definuji jako
budu dále nazývat klouzavým rozptylem a značit $D_t$.

Nechť $\vec{X} = (x_1, x_2, ...,x_n)$ je vektor naměřených hodnot, pak definuji úsekový rozptyl pro $t \in 1, 2, ..., n$ jako 
\begin{equation}
D_m = \frac{1}{\tilde{\omega}} \sum_{k=t-\tilde{\omega}}^{t} (x_k - \widetilde{X}_t)^2,
\label{klouzavý rozptyl}
\end{equation}
kde $\tilde{ \omega } = min\{ \omega, t \}$, $w$ je opět délka úseku a $\widetilde{X}_t$ je klouzavý průměr \eqref{klouzavý průměr}. Pak vektor $\vec{D} = (D_1, D_2, ..., D_n)$ je rys vektoru $\vec{X}$.

\begin{figure} [H] 
	\renewcommand{\figurename}{Obr.}
	\centering
	\includegraphics[scale=0.7]{Obr/obr3}
	\caption{Rozdíl mezi úsekovým a normálním rozptylem aplikovaným na syntetická data}
	\label{obr3}
\end{figure}

\pagestyle{headings}

\section{Způsoby vyhodnocení výsledků}
%V této kapitole se budu věnovat způsobům jak ohodnotit kvalitu (přsnost) mého algoritmu.
\subsection{Přesnost}
\subsection{Správnost}
\subsection{Recall}
\subsection{F míra}

\begin{comment}
\begin{table}
	\scriptsize
	\begin{tabular}{lcrrrrrrrrrrrr}
		\toprule 
		Kombinace rysů &  délka úseku &  Accuracy &  Chyby &  F míra stavu 0 &  F míra stavu 1 &  F míra stavu 2 &  F míra průměrná &  Precision stavu 0 &  Precision stavu 1 &  Precision stavu 2 &  Recall stavu 0 &  Recall stavu 1 &  Recall stavu 2 \\
		\midrule   
		(0, 0, 1, 0, 1) &           16 &  0.866000 &    402 &        0.938991 &        0.750916 &        0.760369 &         0.816758 &           0.939512 &           0.613772 &           0.933962 &        0.938470 &        0.966981 &        0.641192 \\
		
		 (0, 0, 1, 0, 1) &           15 &  0.867333 &    398 &        0.942991 &        0.754786 &        0.748428 &         0.815401 &           0.937055 &           0.615156 &           0.952000 &        0.949002 &        0.976415 &        0.616580 \\
		 
		(0, 0, 0, 1, 1) &           18 &  0.856333 &    431 &        0.924212 &        0.734222 &        0.781065 &         0.813167 &           0.947062 &           0.589158 &           0.910345 &        0.902439 &        0.974057 &        0.683938 \\
		
		(0, 0, 1, 0, 1) &           17 &  0.862000 &    414 &        0.936087 &        0.739574 &        0.762332 &         0.812664 &           0.942664 &           0.609160 &           0.901060 &        0.929601 &        0.941038 &        0.660622 \\
		
		(0, 0, 0, 1, 1) &           16 &  0.861333 &    416 &        0.935159 &        0.736842 &        0.765565 &         0.812522 &           0.943067 &           0.592539 &           0.941399 &        0.927384 &        0.974057 &        0.645078 \\
		
		(0, 0, 0, 1, 1) &           17 &  0.858000 &    426 &        0.928632 &        0.732919 &        0.775602 &         0.812385 &           0.945434 &           0.587482 &           0.926259 &        0.912417 &        0.974057 &        0.667098 \\
		
		(0, 0, 1, 0, 1) &           14 &  0.866333 &    401 &        0.943777 &        0.760770 &        0.730924 &         0.811824 &           0.929570 &           0.622189 &           0.961945 &        0.958426 &        0.978774 &        0.589378 \\
		
		(0, 0, 0, 1, 1) &           19 &  0.853667 &    439 &        0.921670 &        0.728738 &        0.782671 &         0.811026 &           0.951594 &           0.587302 &           0.884176 &        0.893570 &        0.959906 &        0.702073 \\
		
		(0, 0, 1, 0, 1) &           18 &  0.859000 &    423 &        0.932884 &        0.735675 &        0.763449 &         0.810669 &           0.945361 &           0.604863 &           0.885470 &        0.920732 &        0.938679 &        0.670984 \\
		
		(0, 0, 0, 1, 1) &           15 &  0.859333 &    422 &        0.937743 &        0.732270 &        0.750392 &         0.806802 &           0.940357 &           0.586648 &           0.952191 &        0.935144 &        0.974057 &        0.619171 \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}
	\scriptsize
	\begin{tabular}{lllllllllll}
		\toprule 
		&                0 &                1 &                2 &                3 &                4 &                5 &                6 &                7 &                8 &                9 \\
		\midrule
		Kombinace rysů    &  (0, 0, 1, 0, 1) &  (0, 0, 1, 0, 1) &  (0, 0, 0, 1, 1) &  (0, 0, 1, 0, 1) &  (0, 0, 0, 1, 1) &  (0, 0, 0, 1, 1) &  (0, 0, 1, 0, 1) &  (0, 0, 0, 1, 1) &  (0, 0, 1, 0, 1) &  (0, 0, 0, 1, 1) \\
		délka úseku       &               16 &               15 &               18 &               17 &               16 &               17 &               14 &               19 &               18 &               15 \\
		Accuracy          &            0.866 &         0.867333 &         0.856333 &            0.862 &         0.861333 &            0.858 &         0.866333 &         0.853667 &            0.859 &         0.859333 \\
		Chyby             &              402 &              398 &              431 &              414 &              416 &              426 &              401 &              439 &              423 &              422 \\
		F míra stavu 0    &         0.938991 &         0.942991 &         0.924212 &         0.936087 &         0.935159 &         0.928632 &         0.943777 &          0.92167 &         0.932884 &         0.937743 \\
		F míra stavu 1    &         0.750916 &         0.754786 &         0.734222 &         0.739574 &         0.736842 &         0.732919 &          0.76077 &         0.728738 &         0.735675 &          0.73227 \\
		F míra stavu 2    &         0.760369 &         0.748428 &         0.781065 &         0.762332 &         0.765565 &         0.775602 &         0.730924 &         0.782671 &         0.763449 &         0.750392 \\
		F míra průměrná   &         0.816758 &         0.815401 &         0.813167 &         0.812664 &         0.812522 &         0.812385 &         0.811824 &         0.811026 &         0.810669 &         0.806802 \\
		Precision stavu 0 &         0.939512 &         0.937055 &         0.947062 &         0.942664 &         0.943067 &         0.945434 &          0.92957 &         0.951594 &         0.945361 &         0.940357 \\
		Precision stavu 1 &         0.613772 &         0.615156 &         0.589158 &          0.60916 &         0.592539 &         0.587482 &         0.622189 &         0.587302 &         0.604863 &         0.586648 \\
		Precision stavu 2 &         0.933962 &            0.952 &         0.910345 &          0.90106 &         0.941399 &         0.926259 &         0.961945 &         0.884176 &          0.88547 &         0.952191 \\
		Recall stavu 0    &          0.93847 &         0.949002 &         0.902439 &         0.929601 &         0.927384 &         0.912417 &         0.958426 &          0.89357 &         0.920732 &         0.935144 \\
		Recall stavu 1    &         0.966981 &         0.976415 &         0.974057 &         0.941038 &         0.974057 &         0.974057 &         0.978774 &         0.959906 &         0.938679 &         0.974057 \\
		Recall stavu 2    &         0.641192 &          0.61658 &         0.683938 &         0.660622 &         0.645078 &         0.667098 &         0.589378 &         0.702073 &         0.670984 &         0.619171 \\
		\bottomrule
	\end{tabular}
\end{table}
\end{comment}

\chapter*{Závěr}

\pagestyle{plain}

\addcontentsline{toc}{chapter}{Závěr}

Text závěru....
\begin{thebibliography}{1}
\bibitem{Bishop} C. M. Bishop, Pattern recognition and machine learning. Springer, New York, 2013.

\bibitem{Kikuchi} M. Kikuchi,  K. Lackner, M. Q. Tran, Fusion physics. International Atomic Energy Agency, Vienna, 2012.

\bibitem{Rabiner} L. R. Rabiner, A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of the IEEE 77(2), 1989, 257-286.

\bibitem{Markov Chain} N. Privault, Understanding Markov Chains: Examples and Applications. Springer, 2013.

\bibitem{Viterbi} G. D. Forney, The Viterbi Algorithm. Proceedings of the IEEE 61(3) 1973, 268-278.

\bibitem{Machine learning in action}P. Harrington, Machine Learning in Action. Minning Publications Co. Greenwich, 2012.

\bibitem{NumRec} W.H. Press, S.A. Teukolsky, W.T. Vetterling, B.F. Flannery, Numerical Recepies in C: The Art of Scientific Computing (Second Edition).  New York: Cambridge University Press. ISBN 0-521-43108-5, 1992.

\bibitem{Plazma} R.J. Goldston, P.H. Rutherford. Introduction to Plasma Physics. Taylor and Francis. ISBN 978-0-7503-0183-1, 1995.

\bibitem{COMPASS}{http://www.ipp.cas.cz/vedecka\_struktura\_ufp/tokamak/tokamak\_compass/index.html}

\bibitem{kmeansviz}{http://stanford.edu/~cpiech/cs221/img/kmeansViz.png}

%\bibitem{comp_foto}{http://www.veda.cz/dwn/5430/72392B\_tokamak-compass-dosahl-svetovych-parametru.jpg}

\bibitem{comp_foto1}{https://www.researchgate.net/profile/Janos\_Egert/publication/283617947/figure/fig1/AS:314833057665024@1452073458382/View-of-the-COMPASS-tokamak-left-and-its-vacuum-chamber-with-diagnostic-ports-right.png}

\end{thebibliography}

\end{document}
